{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONE HOT NEURAL NETWORK (OHNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present here One Hot Neural Network, which works as following\n",
    "\n",
    "There are $K$ dense blocks that receive input data, each with the same number of neurons $N$. Each neuron has $m$ weights (one for each input feature of the data); however, only the one with highest absolute value is kept, while others are set to zero. So, for each block, each neuron selects a feature and assigns a weight to it. At this point, each block is summed up, with the addition of a bias, o form a rule as:\n",
    "\n",
    "Block $k$ has rule\n",
    "\n",
    "$r_k = w_{k,1} * a_{\\beta_1} + w_{k, 2} * a_{\\beta_2} + ... + w_{k,N} * a_{\\beta_N} + b_k$\n",
    "\n",
    "Where $a_{\\beta_n}$ indicates the feature selected by neuron $n$, associated with the respective weight $w_{k,n}$.\n",
    "\n",
    "Each rule passes through a sigmoid activation function, which we assume it captures the probability to respect the rule.\n",
    "\n",
    "$p(r_k = 1) = Sigmoid(r_k)$\n",
    "\n",
    "Each rule probability from $K$ blocks is then weighted by a single weight, called $W_{f_k}$ for probability of rule $k$. After assigning weight to each rule, all rules are summed:\n",
    "\n",
    "$Out = W_{f_1} * p(r_1) + W_{f_2} * p(r_2) + ... + W_{f_K} * p(r_K)$\n",
    "\n",
    "Result is finally passed through a sigmoid function to obtain probability of output being 1\n",
    "\n",
    "$p(Out = 1) = Sigmoid(Out)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Block\n",
    "\n",
    "The dense block is set with N neurons, so, defined with $W$ as the weight matrix, it will have dimensions $m \\times N$, with $m$ being the number of input features. In the forward pass, instead of having $m$ weights with different values, each neuron will have the weight with the highest absolute value that retains its value, while the other $m-1$ weights are set to zero. The output of a single neuron changes from:\n",
    "\n",
    "$w_1 * a_1 + w_2 * a_2 + ... + w_m * a_m$\n",
    "\n",
    "to\n",
    "\n",
    "$w_i * a_i$\n",
    "\n",
    "Where $i$ corresponds to the weight with the highest absolute value. However, it should be noted that during backpropagation, this modification is ignored, so it is possible that at some point during training, a weight that surpasses the absolute value of weight $i$ takes its place, while the latter becomes zero.\n",
    "\n",
    "\n",
    "### Aggregation of the Dense Block: Rule\n",
    "\n",
    "Through the aggregation of the output of the dense block, it is possible to obtain a rule of the form:\n",
    "\n",
    "$r_k = w_{k,1} * a_{\\beta_1} + w_{k,2} * a_{\\beta_2} + ... + w_{k,N} * a_{\\beta_N} + b_k$\n",
    "\n",
    "Where $a_{\\beta_n}$ indicates the feature selected by neuron $n$, associated with the respective weight $w_{k,n}$.\n",
    "\n",
    "Each rule passes through a sigmoid activation function, which we assume it captures the probability to respect the rule.\n",
    "\n",
    "$p(r_k = 1) = Sigmoid(r_k)$\n",
    "\n",
    "\n",
    "### Weighting of Rules\n",
    "\n",
    "At this point we have $K$ probabilities of comply respective rule. For each of these probabilities, a weight is assigned, called $W_{f_k}$ for rule $k$. \n",
    "\n",
    "$Out = W_{f_1} * p(r_1) + W_{f_2} * p(r_2) + ... + W_{f_K} * p(r_K)$\n",
    "\n",
    "Notice we applied L1 regularization to rule's weights, such that rules adding little information or redundant rules are invited to be ignored (i.e. zero weight).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import fede_code.torch_functions as tof\n",
    "import fede_code.objects as obj\n",
    "import fede_code.model_trainer as mt\n",
    "import fede_code.graphics_functions as grafun\n",
    "from torch.optim import AdamW, RMSprop\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from fede_code.loss_functions import penalizedLoss\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. HEARTH DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
      "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
      "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
      "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
      "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
      "\n",
      "   ca  thal  target  \n",
      "0   0     1       1  \n",
      "1   0     2       1  \n",
      "2   0     2       1  \n",
      "3   0     2       1  \n",
      "4   0     2       1  \n"
     ]
    }
   ],
   "source": [
    "data_path = 'C:\\\\Users\\\\drikb\\\\Desktop\\\\CRISP\\\\XAI project\\\\nn\\\\data\\\\heart.csv'\n",
    "\n",
    "headers = ['age', 'sex', 'chest_pain', 'resting_blood_pressure',\n",
    "           'serum_cholestoral', 'fasting_blood_sugar', 'resting_ecg_results',\n",
    "           'max_heart_rate_achieved', 'exercise_induced_angina', 'oldpeak', \"slope of the peak\",\n",
    "           'num_of_major_vessels', 'thal', 'heart_disease']\n",
    "\n",
    "df_heart = pd.read_csv(data_path)\n",
    "\n",
    "print(df_heart.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_heart.drop(columns=['target'])\n",
    "y = df_heart['target'].values\n",
    "\n",
    "hearth_x_train, hearth_x_test, hearth_y_train, hearth_y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#standardize the dataset\n",
    "sc = StandardScaler()\n",
    "sc.fit(hearth_x_train)\n",
    "hearth_x_train = sc.transform(hearth_x_train)\n",
    "hearth_x_test = sc.transform(hearth_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hearth_train_set = mt.TrainDataset(torch.tensor(hearth_x_train, dtype=torch.float32),\n",
    "                     torch.tensor(hearth_y_train, dtype=torch.float32))\n",
    "\n",
    "hearth_test_set = mt.TrainDataset(torch.tensor(hearth_x_test, dtype=torch.float32),\n",
    "                           torch.tensor(hearth_y_test, dtype=torch.float32))\n",
    "\n",
    "hearth_train_loader = DataLoader(hearth_train_set, batch_size=32, shuffle=True)\n",
    "hearth_test_loader = DataLoader(hearth_test_set, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PREDICTIVE MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 on 0 score SVM: 0.8316831683168315\n",
      "f1 on 1 score SVM: 0.7901234567901235\n",
      "accuracy SVM: 0.8131868131868132\n"
     ]
    }
   ],
   "source": [
    "svm_clf = svm.SVC(kernel='linear')\n",
    "svm_clf.fit(hearth_x_train,hearth_y_train)\n",
    "predicted_svm = svm_clf.predict(hearth_x_test)\n",
    "f1_0_svm, f1_1_svm = f1_score(hearth_y_test,predicted_svm,\n",
    "                                 average = None,\n",
    "                                zero_division = 0,\n",
    "                                labels = np.array([0, 1]))\n",
    "accuracy_svm = accuracy_score(hearth_y_test, predicted_svm)\n",
    "\n",
    "print('f1 on 0 score SVM:', f1_1_svm)\n",
    "print('f1 on 1 score SVM:', f1_0_svm)\n",
    "print('accuracy SVM:', accuracy_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset of predictions to perform explanation\n",
    "pred_set_svm = mt.TrainDataset(torch.tensor(hearth_x_test, dtype=torch.float32),\n",
    "                               torch.tensor(predicted_svm, dtype=torch.float32))\n",
    "\n",
    "pred_loader_svm = DataLoader(pred_set_svm, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Deep Neural Network (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deepNN(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(deepNN, self).__init__()\n",
    "        self.linear_1 = nn.Linear(in_features, 128)\n",
    "        self.act_1 = nn.LeakyReLU()\n",
    "        self.linear_2 = nn.Linear(128, 64)\n",
    "        self.act_2 = nn.LeakyReLU()\n",
    "        self.linear_out = nn.Linear(64, 1)\n",
    "        self.final_act = nn.Sigmoid()\n",
    "        self.num_rules = 0 #<--- this allows to use model trainer, must be fixed\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.act_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.act_2(x)\n",
    "        x = self.linear_out(x)\n",
    "        x = self.final_act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa3f9d2de654ce2abbb5c666b1529cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dnn = deepNN(in_features = 13)\n",
    "\n",
    "loss_fn = penalizedLoss(loss_fn = nn.BCELoss(reduction='sum'), #<--- reduction='sum' is mandatory\n",
    "                        parameters = dnn.parameters(),\n",
    "                        l1_lambda = 0.5,\n",
    "                        l2_lambda = 0.5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print('GPU')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('CPU')\n",
    "\n",
    "\n",
    "trainer = mt.modelTrainer(model=dnn,\n",
    "                          loss_fn=loss_fn,\n",
    "                          optimizer=AdamW)\n",
    "\n",
    "trainer.train_model(train_data_loader=hearth_train_loader,\n",
    "                    num_epochs=1000,\n",
    "                    device=device,\n",
    "                    learning_rate=1e-5,\n",
    "                    display_log='epoch',\n",
    "                   store_weights_history=False,\n",
    "                   store_weights_grad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169673347cb8429991774636203cf896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 0.4506212590814947\n",
      "Accuracy: 0.8461538461538461\n",
      "F1 score on label 0: 0.815311004784689\n",
      "F1 score on label 1: 0.8612993584530586\n"
     ]
    }
   ],
   "source": [
    "loss_dnn, accuracy_dnn, f1_0_dnn, f1_1_dnn = trainer.eval_model(hearth_test_loader, device=device)\n",
    "print(f'Average loss: {loss_dnn}')\n",
    "print(f'Accuracy: {accuracy_dnn}')\n",
    "print(f'F1 score on label 0: {f1_0_dnn}')\n",
    "print(f'F1 score on label 1: {f1_1_dnn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0133d266c73b40608b63335c3bc15913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_dnn = trainer.predict(hearth_test_loader, device=device, prob=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset of predictions to perform explanation\n",
    "pred_set_dnn = mt.TrainDataset(torch.tensor(hearth_x_test, dtype=torch.float32),\n",
    "                               torch.tensor(predicted_dnn.detach().numpy(), dtype=torch.float32))\n",
    "\n",
    "pred_loader_dnn = DataLoader(pred_set_dnn, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EXPLANATION MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 LOGISTIC REGRESSION (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "\n",
    "log_reg_svm = log_reg.fit(hearth_x_test, predicted_svm)\n",
    "log_reg_dnn = log_reg.fit(hearth_x_test, predicted_dnn)\n",
    "\n",
    "log_reg_pred_svm = log_reg_svm.predict(hearth_x_test)\n",
    "log_reg_pred_dnn = log_reg_dnn.predict(hearth_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation accuracy on SVM: 0.967032967032967\n",
      "Explanation f1 of 0 on SVM: 0.9629629629629629\n",
      "Explanation f1 of 1 on SVM: 0.9702970297029702\n",
      "Explanation accuracy on DNN: 1.0\n",
      "Explanation f1 of 0 on DNN: 1.0\n",
      "Explanation f1 of 1 on DNN: 1.0\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "log_reg_accuracy_svm = (log_reg_pred_svm == predicted_svm).sum()/len(predicted_svm)\n",
    "log_reg_f1_0_svm, log_reg_f1_1_svm = f1_score(predicted_svm,log_reg_pred_svm,\n",
    "                                              average = None,\n",
    "                                              zero_division = 0,\n",
    "                                              labels = np.array([0, 1]))\n",
    "\n",
    "# DNN\n",
    "log_reg_accuracy_dnn = (log_reg_pred_dnn == predicted_dnn.detach().numpy()).sum()/len(predicted_dnn)\n",
    "log_reg_f1_0_dnn, log_reg_f1_1_dnn = f1_score(predicted_dnn,log_reg_pred_dnn,\n",
    "                                              average = None,\n",
    "                                              zero_division = 0,\n",
    "                                              labels = np.array([0, 1]))\n",
    "\n",
    "print('Explanation accuracy on SVM:', log_reg_accuracy_svm)\n",
    "print('Explanation f1 of 0 on SVM:', log_reg_f1_0_svm)\n",
    "print('Explanation f1 of 1 on SVM:', log_reg_f1_1_svm)\n",
    "\n",
    "print('Explanation accuracy on DNN:', log_reg_accuracy_dnn)\n",
    "print('Explanation f1 of 0 on DNN:', log_reg_f1_0_dnn)\n",
    "print('Explanation f1 of 1 on DNN:', log_reg_f1_1_dnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 DECISION TREE (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(criterion='gini',\n",
    "                            max_depth = 4,\n",
    "                            max_leaf_nodes = 16)\n",
    "\n",
    "dt_svm = dt.fit(hearth_x_test, predicted_svm)\n",
    "dt_dnn = dt.fit(hearth_x_test, predicted_dnn)\n",
    "\n",
    "dt_pred_svm = dt_svm.predict(hearth_x_test)\n",
    "dt_pred_dnn = dt_dnn.predict(hearth_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation accuracy on SVM: 0.967032967032967\n",
      "Explanation f1 of 0 on SVM: 0.9629629629629629\n",
      "Explanation f1 of 1 on SVM: 0.9702970297029702\n",
      "Explanation accuracy on DNN: 0.9560439560439561\n",
      "Explanation f1 of 0 on DNN: 0.9512195121951219\n",
      "Explanation f1 of 1 on DNN: 0.96\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "dt_accuracy_svm = (dt_pred_svm == predicted_svm).sum()/len(predicted_svm)\n",
    "dt_f1_0_svm, dt_f1_1_svm = f1_score(predicted_svm,dt_pred_svm,\n",
    "                                              average = None,\n",
    "                                              zero_division = 0,\n",
    "                                              labels = np.array([0, 1]))\n",
    "\n",
    "# DNN\n",
    "dt_accuracy_dnn = (dt_pred_dnn == predicted_dnn.detach().numpy()).sum()/len(predicted_dnn)\n",
    "dt_f1_0_dnn, dt_f1_1_dnn = f1_score(predicted_dnn,dt_pred_dnn,\n",
    "                                              average = None,\n",
    "                                              zero_division = 0,\n",
    "                                              labels = np.array([0, 1]))\n",
    "\n",
    "print('Explanation accuracy on SVM:', dt_accuracy_svm)\n",
    "print('Explanation f1 of 0 on SVM:', dt_f1_0_svm)\n",
    "print('Explanation f1 of 1 on SVM:', dt_f1_1_svm)\n",
    "\n",
    "print('Explanation accuracy on DNN:', dt_accuracy_dnn)\n",
    "print('Explanation f1 of 0 on DNN:', dt_f1_0_dnn)\n",
    "print('Explanation f1 of 1 on DNN:', dt_f1_1_dnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONE HOT NEURAL NETWORK (OHNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2781169395f745ab8422bf77da611379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ohnn_svm = obj.oneHotRuleNN(input_size=13,\n",
    "                        num_neurons=4, #tree max depth\n",
    "                        num_rules=16, #tree max leaves\n",
    "                        final_activation=nn.Sigmoid,\n",
    "                        rule_weight_constraint=tof.getMax,\n",
    "                        out_weight_constraint=None,\n",
    "                        rule_bias=True,\n",
    "                        neuron_bias=False,\n",
    "                        rule_activation=nn.Sigmoid,\n",
    "                        neuron_activation=None,\n",
    "                        out_bias=False,\n",
    "                        force_positive_hidden_init=False,\n",
    "                        force_positive_out_init=False,\n",
    "                        dtype=torch.float32)\n",
    "\n",
    "#loss_fn = nn.BCELoss(reduction='sum')\n",
    "loss_fn = penalizedLoss(loss_fn = nn.BCELoss(reduction='sum'), #<--- reduction='sum' is mandatory\n",
    "                        parameters = ohnn_svm.rule_weight.linear.weight,\n",
    "                        l1_lambda = 0.3,\n",
    "                        l2_lambda = 0.0)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print('GPU')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('CPU')\n",
    "\n",
    "\n",
    "trainer_svm = mt.modelTrainer(model=ohnn_svm,\n",
    "                          loss_fn=loss_fn,\n",
    "                          optimizer=AdamW)\n",
    "\n",
    "trainer_svm.train_model(train_data_loader=pred_loader_svm,\n",
    "                    num_epochs=1000,\n",
    "                    device=device,\n",
    "                    learning_rate=1e-3,\n",
    "                    display_log='epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a955b96c88ab43668f0f0bdd33eaec99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ohnn_dnn = obj.oneHotRuleNN(input_size=13,\n",
    "                        num_neurons=4, #tree max depth\n",
    "                        num_rules=16, #tree max leaves\n",
    "                        final_activation=nn.Sigmoid,\n",
    "                        rule_weight_constraint=tof.getMax,\n",
    "                        out_weight_constraint=None,\n",
    "                        rule_bias=True,\n",
    "                        neuron_bias=False,\n",
    "                        rule_activation=nn.Sigmoid,\n",
    "                        neuron_activation=None,\n",
    "                        out_bias=False,\n",
    "                        force_positive_hidden_init=False,\n",
    "                        force_positive_out_init=False,\n",
    "                        dtype=torch.float32)\n",
    "\n",
    "#loss_fn = nn.BCELoss(reduction='sum')\n",
    "loss_fn = penalizedLoss(loss_fn = nn.BCELoss(reduction='sum'), #<--- reduction='sum' is mandatory\n",
    "                        parameters = ohnn_dnn.rule_weight.linear.weight,\n",
    "                        l1_lambda = 0.3,\n",
    "                        l2_lambda = 0.0)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print('GPU')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('CPU')\n",
    "\n",
    "\n",
    "trainer_dnn = mt.modelTrainer(model=ohnn_dnn,\n",
    "                          loss_fn=loss_fn,\n",
    "                          optimizer=AdamW)\n",
    "\n",
    "trainer_dnn.train_model(train_data_loader=pred_loader_dnn,\n",
    "                    num_epochs=1000,\n",
    "                    device=device,\n",
    "                    learning_rate=1e-3,\n",
    "                    display_log='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40524175d1f49a9a1a85fa9fb8cc97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03ef995f4f34807b299a9b6b2f3b5bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation accuracy on SVM: 1.0\n",
      "Explanation f1 of 0 on SVM: 1.0\n",
      "Explanation f1 of 1 on SVM: 1.0\n",
      "Explanation accuracy on DNN: 0.967032967032967\n",
      "Explanation f1 of 0 on DNN: 0.9633699633699634\n",
      "Explanation f1 of 1 on DNN: 0.9727095516569202\n"
     ]
    }
   ],
   "source": [
    "ohnn_loss_svm, ohnn_accuracy_svm, ohnn_f1_0_svm, ohnn_f1_1_svm = trainer_svm.eval_model(pred_loader_svm, device='cpu')\n",
    "ohnn_loss_dnn, ohnn_accuracy_dnn, ohnn_f1_0_dnn, ohnn_f1_1_dnn = trainer_dnn.eval_model(pred_loader_dnn, device='cpu')\n",
    "\n",
    "print('Explanation accuracy on SVM:', ohnn_accuracy_svm)\n",
    "print('Explanation f1 of 0 on SVM:', ohnn_f1_0_svm)\n",
    "print('Explanation f1 of 1 on SVM:', ohnn_f1_1_svm)\n",
    "\n",
    "print('Explanation accuracy on DNN:', ohnn_accuracy_dnn)\n",
    "print('Explanation f1 of 0 on DNN:', ohnn_f1_0_dnn)\n",
    "print('Explanation f1 of 1 on DNN:', ohnn_f1_1_dnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\n",
    "    'Logistic regression': {\n",
    "        'SVM': {'Accuracy': log_reg_accuracy_svm, 'F1 Score (Class 0)': log_reg_f1_0_svm, 'F1 Score (Class 1)': log_reg_f1_1_svm},\n",
    "        'DNN': {'Accuracy': log_reg_accuracy_dnn, 'F1 Score (Class 0)': log_reg_f1_0_dnn, 'F1 Score (Class 1)': log_reg_f1_1_dnn},\n",
    "    },\n",
    "    'Decision tree': {\n",
    "        'SVM': {'Accuracy': dt_accuracy_svm, 'F1 Score (Class 0)': dt_f1_0_svm, 'F1 Score (Class 1)': dt_f1_1_svm},\n",
    "        'DNN': {'Accuracy': dt_accuracy_dnn, 'F1 Score (Class 0)': dt_f1_0_dnn, 'F1 Score (Class 1)': dt_f1_1_dnn},\n",
    "    },\n",
    "    'OHNN': {\n",
    "        'SVM': {'Accuracy': ohnn_accuracy_svm, 'F1 Score (Class 0)': ohnn_f1_0_svm, 'F1 Score (Class 1)': ohnn_f1_1_svm},\n",
    "        'DNN': {'Accuracy': ohnn_accuracy_dnn, 'F1 Score (Class 0)': ohnn_f1_0_dnn, 'F1 Score (Class 1)': ohnn_f1_1_dnn},\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------------------+----------+--------------------+--------------------+\n",
      "|        Model        | Explained Predictions | Accuracy | F1 Score (Class 0) | F1 Score (Class 1) |\n",
      "+---------------------+-----------------------+----------+--------------------+--------------------+\n",
      "| Logistic regression |          SVM          |  0.967   |       0.963        |       0.9703       |\n",
      "| Logistic regression |          DNN          |   1.0    |        1.0         |        1.0         |\n",
      "|    Decision tree    |          SVM          |  0.967   |       0.963        |       0.9703       |\n",
      "|    Decision tree    |          DNN          |  0.956   |       0.9512       |        0.96        |\n",
      "|        OHNN         |          SVM          |   1.0    |        1.0         |        1.0         |\n",
      "|        OHNN         |          DNN          |  0.967   |       0.9634       |       0.9727       |\n",
      "+---------------------+-----------------------+----------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "df_scores = pd.DataFrame.from_dict({(i, j): scores[i][j] \n",
    "                              for i in scores.keys() \n",
    "                              for j in scores[i].keys()}, orient='index')\n",
    "\n",
    "# Rename index and columns\n",
    "df_scores.index.names = ['Model', 'Explained Predictions']\n",
    "df_scores.reset_index(inplace=True)\n",
    "\n",
    "# Format the DataFrame to round the values to a certain number of decimal places (e.g., 2)\n",
    "df_scores = df_scores.round(4)\n",
    "\n",
    "# Print the table using tabulate\n",
    "table = tabulate(df_scores, headers='keys', tablefmt='pretty', showindex=False)\n",
    "\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
